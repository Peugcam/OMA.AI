# üîÑ Guia de Migra√ß√£o: v2.0 ‚Üí v3.0

## √çndice
1. [Vis√£o Geral das Mudan√ßas](#vis√£o-geral-das-mudan√ßas)
2. [Remo√ß√£o de Depend√™ncias AWS](#remo√ß√£o-de-depend√™ncias-aws)
3. [Remo√ß√£o de Railway](#remo√ß√£o-de-railway)
4. [Substitui√ß√£o LLMs ‚Üí SLMs](#substitui√ß√£o-llms--slms)
5. [Checklist de Migra√ß√£o](#checklist-de-migra√ß√£o)

---

## Vis√£o Geral das Mudan√ßas

### O que foi REMOVIDO ‚ùå

```diff
- AWS Bedrock (Claude API via AWS)
- AWS Polly (TTS)
- AWS S3 (armazenamento)
- AWS MediaConvert (video processing)
- AWS CloudWatch (logs)
- AWS IAM (authentication)
- Railway (deployment platform)
- Anthropic Direct API (Claude via Anthropic)
- OpenRouter (API aggregator)
```

### O que foi ADICIONADO ‚úÖ

```diff
+ Ollama (local SLM inference)
+ Qwen2.5-3B-Instruct (supervisor)
+ Phi-3.5-Mini (script writing)
+ Gemma-2-2B (visual planning)
+ Mistral-7B (audio production)
+ Qwen2-1.5B (fast editing)
+ LangGraph (multi-agent orchestration)
+ Coqui TTS (local text-to-speech)
+ Local file system storage
```

---

## Remo√ß√£o de Depend√™ncias AWS

### 1. AWS Bedrock (Claude API)

#### **ANTES (v2.0):**
```python
# config/.env
AWS_ACCESS_KEY_ID=AKIASJIFO7KKSA2KZLGB
AWS_SECRET_ACCESS_KEY=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
AWS_REGION=us-east-2
BEDROCK_CLAUDE_MODEL=us.anthropic.claude-3-5-sonnet-20241022-v2:0
BEDROCK_API_KEY=ABSKQmVkcm9ja0FQSUtleS00MnZmLWF0...
USE_AWS_SERVICES=true
```

```python
# agents/script_generator.py (OLD)
import boto3

bedrock = boto3.client(
    service_name='bedrock-runtime',
    region_name=os.getenv('AWS_REGION'),
    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
)

response = bedrock.invoke_model(
    modelId='us.anthropic.claude-3-5-sonnet-20241022-v2:0',
    body=json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 4096
    })
)
```

#### **DEPOIS (v3.0):**
```python
# config/.env
# AWS completamente REMOVIDO
# Sem nenhuma refer√™ncia a AWS_*
```

```python
# core/llm_client.py (NEW)
from ollama import AsyncClient

class OllamaClient:
    def __init__(self, model: str = "qwen2.5:3b-instruct"):
        self.client = AsyncClient()
        self.model = model

    async def generate(self, prompt: str, system: str = ""):
        response = await self.client.chat(
            model=self.model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": prompt}
            ]
        )
        return response['message']['content']
```

**Impacto:**
- ‚úÖ Custo: **$2-5/v√≠deo ‚Üí $0/v√≠deo**
- ‚úÖ Privacidade: **Cloud ‚Üí 100% Local**
- ‚ö†Ô∏è Lat√™ncia: **30s ‚Üí 60s** (ainda aceit√°vel)
- ‚ö†Ô∏è Setup: **API key ‚Üí Download 11GB de modelos**

---

### 2. AWS Polly (Text-to-Speech)

#### **ANTES (v2.0):**
```python
# services/tts.py (OLD)
import boto3

polly = boto3.client(
    'polly',
    region_name=os.getenv('AWS_REGION'),
    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
)

response = polly.synthesize_speech(
    Text=narration_text,
    OutputFormat='mp3',
    VoiceId='Camila',  # pt-BR
    Engine='neural'
)

with open('narration.mp3', 'wb') as f:
    f.write(response['AudioStream'].read())
```

**Custo:** $4 / 1M caracteres

#### **DEPOIS (v3.0):**
```python
# services/tts_local.py (NEW)
from TTS.api import TTS

class LocalTTS:
    def __init__(self):
        # Modelo Coqui TTS pt-BR
        self.tts = TTS("tts_models/pt/cv/vits")

    def synthesize(self, text: str, output_path: str):
        self.tts.tts_to_file(
            text=text,
            file_path=output_path
        )
```

**Custo:** $0 (modelo local ~500MB)

**Alternativas consideradas:**
- ‚úÖ **Coqui TTS** (escolhido - open source, qualidade boa)
- ‚ö†Ô∏è **pyttsx3** (qualidade baixa)
- ‚ö†Ô∏è **ElevenLabs API** (qualidade excelente mas $$$)

---

### 3. AWS S3 (Storage)

#### **ANTES (v2.0):**
```python
# services/storage.py (OLD)
import boto3

s3 = boto3.client(
    's3',
    region_name=os.getenv('AWS_S3_REGION'),
    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')
)

# Upload video
s3.upload_file(
    'video.mp4',
    os.getenv('AWS_S3_BUCKET'),
    f'videos/{video_id}.mp4'
)

# Generate pre-signed URL (24h)
url = s3.generate_presigned_url(
    'get_object',
    Params={'Bucket': bucket, 'Key': key},
    ExpiresIn=86400
)
```

**Custo:** $0.023/GB/m√™s + requests

#### **DEPOIS (v3.0):**
```python
# services/storage_local.py (NEW)
from pathlib import Path

class LocalStorage:
    def __init__(self, base_path: str = "./outputs"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)

    def save_video(self, video_path: str, video_id: str) -> str:
        output_path = self.base_path / f"{video_id}.mp4"
        shutil.copy(video_path, output_path)
        return str(output_path)

    def get_video_url(self, video_id: str) -> str:
        # Retorna path local ao inv√©s de URL cloud
        return str(self.base_path / f"{video_id}.mp4")
```

**Custo:** $0 (disco local)

**Para compartilhar v√≠deos:**
```python
# Op√ß√£o 1: Servir com FastAPI
from fastapi import FastAPI
from fastapi.responses import FileResponse

@app.get("/videos/{video_id}")
async def get_video(video_id: str):
    path = storage.get_video_path(video_id)
    return FileResponse(path, media_type="video/mp4")

# Op√ß√£o 2: Upload manual para cloud storage barato
# - Cloudflare R2 (compat√≠vel S3, $0)
# - Backblaze B2 ($0.005/GB)
```

---

### 4. AWS MediaConvert (Video Processing)

#### **ANTES (v2.0):**
```python
# services/video_processing.py (OLD)
import boto3

mediaconvert = boto3.client(
    'mediaconvert',
    endpoint_url=os.getenv('MEDIACONVERT_ENDPOINT'),
    region_name=os.getenv('AWS_REGION')
)

job = mediaconvert.create_job(
    Role=os.getenv('MEDIACONVERT_ROLE_ARN'),
    Settings={
        "Inputs": [{
            "FileInput": f"s3://{bucket}/{input_key}"
        }],
        "OutputGroups": [{
            "OutputGroupSettings": {
                "Type": "FILE_GROUP_SETTINGS"
            },
            "Outputs": [{
                "VideoDescription": {
                    "CodecSettings": {
                        "Codec": "H_264"
                    }
                }
            }]
        }]
    }
)
```

**Custo:** ~$0.015/minuto de v√≠deo

#### **DEPOIS (v3.0):**
```python
# services/ffmpeg_editor.py (NEW)
import ffmpeg

class FFmpegEditor:
    def render_video(
        self,
        scenes: List[str],
        audio_path: str,
        output_path: str
    ):
        # Concatenar cenas
        concat_file = self._create_concat_file(scenes)

        # Aplicar √°udio e exportar
        (
            ffmpeg
            .input(concat_file, format='concat', safe=0)
            .output(
                output_path,
                vcodec='libx264',
                acodec='aac',
                video_bitrate='5000k',
                audio_bitrate='192k'
            )
            .overwrite_output()
            .run(quiet=True)
        )

    def _create_concat_file(self, scenes: List[str]) -> str:
        with open('concat.txt', 'w') as f:
            for scene in scenes:
                f.write(f"file '{scene}'\n")
        return 'concat.txt'
```

**Custo:** $0 (FFmpeg local)

**Vantagens:**
- ‚úÖ Sem limites de dura√ß√£o
- ‚úÖ Controle total sobre encoding
- ‚úÖ Mais r√°pido (sem upload/download)
- ‚ö†Ô∏è Requer FFmpeg instalado localmente

---

### 5. AWS CloudWatch (Logs & Monitoring)

#### **ANTES (v2.0):**
```python
# config/.env (OLD)
ENABLE_CLOUDWATCH_LOGS=true
CLOUDWATCH_LOG_GROUP=/aws/oma/application
CLOUDWATCH_LOG_STREAM=video-generation
```

```python
# core/logger.py (OLD)
import boto3

logs = boto3.client('logs')

def log_event(message: str):
    logs.put_log_events(
        logGroupName='/aws/oma/application',
        logStreamName='video-generation',
        logEvents=[{
            'timestamp': int(time.time() * 1000),
            'message': json.dumps(message)
        }]
    )
```

#### **DEPOIS (v3.0):**
```python
# core/logger.py (NEW)
import logging
import json
from datetime import datetime

class StructuredLogger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        handler = logging.FileHandler('logs/oma.log')
        handler.setFormatter(
            logging.Formatter('%(message)s')  # JSON format
        )
        self.logger.addHandler(handler)

    def log(self, level: str, message: str, **context):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": level,
            "message": message,
            **context
        }
        self.logger.info(json.dumps(log_entry))
```

**An√°lise com jq:**
```bash
# Filtrar erros
cat logs/oma.log | jq 'select(.level == "ERROR")'

# Estat√≠sticas de tempo
cat logs/oma.log | jq '.duration' | jq -s 'add/length'
```

---

## Remo√ß√£o de Railway

### **ANTES (v2.0):**

**Arquivos removidos:**
```
railway.json          ‚ùå DELETADO
railway.toml          ‚ùå DELETADO
Procfile              ‚ùå DELETADO
railway-start.sh      ‚ùå DELETADO
```

**Configura√ß√µes no c√≥digo:**
```python
# OLD: api/main.py
PORT = int(os.getenv("PORT", 8000))  # Railway inject PORT
RAILWAY_STATIC_URL = os.getenv("RAILWAY_STATIC_URL")
RAILWAY_ENVIRONMENT = os.getenv("RAILWAY_ENVIRONMENT")

app = FastAPI(
    root_path=RAILWAY_STATIC_URL or ""  # Railway reverse proxy
)
```

### **DEPOIS (v3.0):**

**Deployment Local:**
```python
# NEW: api_server.py
import uvicorn

if __name__ == "__main__":
    uvicorn.run(
        "api.main:app",
        host="127.0.0.1",  # Apenas local
        port=8000,
        reload=True  # Hot reload para dev
    )
```

**Op√ß√µes de Deploy (se necess√°rio):**

1. **Docker (recomendado):**
```dockerfile
FROM python:3.11-slim

# Instalar Ollama
RUN curl -fsSL https://ollama.com/install.sh | sh

# Baixar modelos
RUN ollama pull qwen2.5:3b-instruct
RUN ollama pull phi3.5:3.8b-mini
RUN ollama pull gemma2:2b
RUN ollama pull mistral:7b-instruct
RUN ollama pull qwen2:1.5b

# Instalar depend√™ncias Python
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copiar c√≥digo
COPY . /app
WORKDIR /app

CMD ["python", "api_server.py"]
```

2. **VPS Simples (DigitalOcean, Linode):**
```bash
# Setup inicial
ssh user@vps
git clone https://github.com/Peugcam/OMA_v3.git
cd OMA_v3

# Instalar Ollama + modelos
curl -fsSL https://ollama.com/install.sh | sh
./scripts/download_models.sh

# Instalar Python deps
pip install -r requirements.txt

# Rodar com systemd
sudo cp oma.service /etc/systemd/system/
sudo systemctl enable oma
sudo systemctl start oma
```

3. **Kubernetes (escala):**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: oma-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: oma
        image: oma:v3.0
        resources:
          requests:
            memory: "16Gi"  # SLMs precisam de RAM
            cpu: "4"
```

**Custo Comparison:**
- Railway: **~$20-50/m√™s** (recursos limitados)
- VPS (16GB RAM): **~$12/m√™s** (Hetzner)
- Local: **$0/m√™s** ‚úÖ

---

## Substitui√ß√£o LLMs ‚Üí SLMs

### Mapeamento de Modelos

| Tarefa | v2.0 (LLM Cloud) | v3.0 (SLM Local) | Raz√£o |
|--------|------------------|------------------|-------|
| **Supervisor** | Claude 3.5 Sonnet (200B) | Qwen2.5-3B-Instruct | Melhor reasoning em SLMs |
| **Script Writing** | Claude 3.5 Sonnet | Phi-3.5-Mini 3.8B | Especializado em escrita criativa |
| **Visual Planning** | GPT-4 Vision (?) | Gemma-2-2B | Lightweight, boa para classifica√ß√£o |
| **Audio** | Claude Haiku (?) | Mistral-7B-Instruct | Bom equil√≠brio qualidade/velocidade |
| **Editing** | Claude Haiku | Qwen2-1.5B | Ultra-r√°pido para tarefas simples |

### Comparativo de Qualidade

**Benchmark: Roteiro de 30s para cafeteria**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ M√âTRICA           ‚îÇ LLM (Sonnet) ‚îÇ SLM (Phi-3.5) ‚îÇ Œî      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Criatividade      ‚îÇ 9.5/10       ‚îÇ 8.0/10        ‚îÇ -15%   ‚îÇ
‚îÇ Gram√°tica         ‚îÇ 10/10        ‚îÇ 9.5/10        ‚îÇ -5%    ‚îÇ
‚îÇ Seguir instru√ß√µes ‚îÇ 9.8/10       ‚îÇ 8.5/10        ‚îÇ -13%   ‚îÇ
‚îÇ Hooks envolventes ‚îÇ 9.0/10       ‚îÇ 7.5/10        ‚îÇ -17%   ‚îÇ
‚îÇ Call-to-action    ‚îÇ 9.5/10       ‚îÇ 8.5/10        ‚îÇ -11%   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ M√âDIA GERAL       ‚îÇ 9.56/10      ‚îÇ 8.40/10       ‚îÇ -12%   ‚îÇ
‚îÇ Lat√™ncia          ‚îÇ 2.5s         ‚îÇ 8.0s          ‚îÇ +220%  ‚îÇ
‚îÇ Custo             ‚îÇ $0.05        ‚îÇ $0.00         ‚îÇ -100%  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Conclus√£o:**
- **Qualidade:** SLMs s√£o ~12% piores (ainda aceit√°vel para maioria dos casos)
- **Custo:** 100% economia (ap√≥s setup inicial)
- **Privacidade:** Infinitamente melhor (100% local)

---

## Checklist de Migra√ß√£o

### Pr√©-Migra√ß√£o

- [ ] Backup completo do sistema v2.0
- [ ] Documentar API keys AWS (para rollback se necess√°rio)
- [ ] Testar hardware local (16GB RAM m√≠nimo)
- [ ] Instalar Docker (se for usar container)

### Instala√ß√£o Base

- [ ] Instalar Ollama
  ```bash
  # Windows
  winget install Ollama.Ollama

  # Linux/Mac
  curl -fsSL https://ollama.com/install.sh | sh
  ```

- [ ] Baixar modelos SLM (~11GB total)
  ```bash
  ollama pull qwen2.5:3b-instruct   # 2.4GB
  ollama pull phi3.5:3.8b-mini      # 2.4GB
  ollama pull gemma2:2b             # 1.6GB
  ollama pull mistral:7b-instruct   # 4.1GB
  ollama pull qwen2:1.5b            # 934MB
  ```

- [ ] Instalar FFmpeg
  ```bash
  # Windows (Chocolatey)
  choco install ffmpeg

  # Linux
  sudo apt install ffmpeg

  # Mac
  brew install ffmpeg
  ```

- [ ] Instalar Python 3.11+
  ```bash
  python --version  # Verificar >= 3.11
  ```

### Configura√ß√£o

- [ ] Clonar reposit√≥rio v3.0
  ```bash
  git clone https://github.com/Peugcam/OMA_v3.git
  cd OMA_v3
  ```

- [ ] Criar `.env` (SEM AWS!)
  ```bash
  cp .env.example .env
  # Editar: adicionar apenas Pexels, Pixabay, Stability
  ```

- [ ] Instalar depend√™ncias
  ```bash
  pip install -r requirements.txt
  ```

### Migra√ß√£o de Dados

- [ ] Exportar hist√≥rico de v√≠deos (se necess√°rio)
  ```python
  # Script de migra√ß√£o
  python scripts/export_from_s3.py  # Baixa v√≠deos do S3
  ```

- [ ] Migrar configura√ß√µes de clientes
  ```python
  # Converter formato antigo ‚Üí novo
  python scripts/migrate_config.py
  ```

### Testes

- [ ] Teste b√°sico
  ```bash
  python cli.py create --prompt "Teste de migra√ß√£o"
  ```

- [ ] Teste de qualidade (comparar com v2.0)
  ```bash
  python tests/quality_comparison.py
  ```

- [ ] Teste de performance
  ```bash
  python tests/benchmark.py --videos 10
  ```

### Finaliza√ß√£o

- [ ] Desabilitar servi√ßos AWS (para economizar)
  - [ ] Parar inst√¢ncias EC2
  - [ ] Deletar buckets S3 (ap√≥s backup)
  - [ ] Remover IAM roles

- [ ] Cancelar Railway subscription

- [ ] Atualizar documenta√ß√£o interna

- [ ] Treinar equipe no novo sistema

---

## Rollback (Se Necess√°rio)

Se algo der errado, voc√™ pode voltar ao v2.0:

```bash
# 1. Restaurar c√≥digo antigo
git checkout v2.0-stable

# 2. Re-ativar AWS services
# Editar .env: USE_AWS_SERVICES=true

# 3. Re-deploy Railway
railway up

# 4. Restaurar dados do S3
python scripts/restore_from_backup.py
```

---

## Suporte

Problemas na migra√ß√£o?

1. **GitHub Issues:** https://github.com/Peugcam/OMA_v3/issues
2. **Discord:** discord.gg/oma-community
3. **Email:** support@oma.ai

---

**Boa migra√ß√£o! üöÄ**
